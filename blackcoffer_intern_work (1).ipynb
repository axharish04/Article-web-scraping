{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "#list of urls\n",
        "urls=[\n",
        "\"https://insights.blackcoffer.com/ml-and-ai-based-insurance-premium-model-to-predict-premium-to-be-charged-by-the-insurance-company/\",\n",
        "\"https://insights.blackcoffer.com/streamlined-integration-interactive-brokers-api-with-python-for-desktop-trading-application/\",\n",
        "\"https://insights.blackcoffer.com/efficient-data-integration-and-user-friendly-interface-development-navigating-challenges-in-web-application-deployment/\",\n",
        "\"https://insights.blackcoffer.com/effective-management-of-social-media-data-extraction-strategies-for-authentication-security-and-reliability/\",\n",
        "\"https://insights.blackcoffer.com/streamlined-trading-operations-interface-for-metatrader-4-empowering-efficient-management-and-monitoring/\",\n",
        "\"https://insights.blackcoffer.com/efficient-aws-infrastructure-setup-and-management-addressing-security-scalability-and-compliance/\",\n",
        "\"https://insights.blackcoffer.com/streamlined-equity-waterfall-calculation-and-deal-management-system/\",\n",
        "\"https://insights.blackcoffer.com/automated-orthopedic-case-report-generation-harnessing-web-scraping-and-ai-integration/\",\n",
        "\"https://insights.blackcoffer.com/streamlining-time-calculation-in-warehouse-management-leveraging-shiphero-api-and-google-bigquery-integration/\",\n",
        "\"https://insights.blackcoffer.com/efficient-database-design-and-management-streamlining-access-and-integration-for-partner-entity-management/\",\n",
        "\"https://insights.blackcoffer.com/automated-campaign-management-system-a-comprehensive-solution-with-linkedin-and-email-integration/\",\n",
        "\"https://insights.blackcoffer.com/ai-driven-data-analysis-ai-tool-using-langchain-for-a-leading-real-estate-and-financing-firm-worldwide/\",\n",
        "\"https://insights.blackcoffer.com/grafana-dashboard-to-visualize-and-analyze-sensors-data/\",\n",
        "\"https://insights.blackcoffer.com/mvp-for-a-software-that-analyses-content-from-audio-pharma-based/\",\n",
        "\"https://insights.blackcoffer.com/data-engineering-and-management-tool-airbyte-with-custom-data-connectors-to-manage-crm-database/\",\n",
        "\"https://insights.blackcoffer.com/text-summarizing-tool-to-scrape-and-summarize-pubmed-medical-papers/\",\n",
        "\"https://insights.blackcoffer.com/7up7down-10updown-snakes-and-ladder-games-built-using-oops/\",\n",
        "\"https://insights.blackcoffer.com/data-studio-dashboard-with-a-data-pipeline-tool-synced-with-podio-using-custom-webhooks-and-google-cloud-function/\",\n",
        "\"https://insights.blackcoffer.com/end-to-end-tool-to-optimize-routing-and-planning-of-field-engineers-using-googles-cvrp-tw-algorithm/\",\n",
        "\"https://insights.blackcoffer.com/end-to-end-tool-to-predict-biofuel-prices-using-ieso-data/\",\n",
        "\"https://insights.blackcoffer.com/etl-discovery-tool-using-llma-langchain-openai/\",\n",
        "\"https://insights.blackcoffer.com/gpt-ocr-api/\",\n",
        "\"https://insights.blackcoffer.com/dockerize-the-aws-lambda-for-serverless-architecture/\",\n",
        "\"https://insights.blackcoffer.com/design-and-develop-a-product-recommendation-engine-based-on-the-features-of-products/\",\n",
        "\"https://insights.blackcoffer.com/database-discovery-tool-using-openai/\",\n",
        "\"https://insights.blackcoffer.com/automate-the-data-management-process/\",\n",
        "\"https://insights.blackcoffer.com/realtime-kibana-dashboard-for-a-financial-tech-firm/\",\n",
        "\"https://insights.blackcoffer.com/data-management-etl-and-data-automation/\",\n",
        "\"https://insights.blackcoffer.com/data-management-egeas/\",\n",
        "\"https://insights.blackcoffer.com/design-and-develop-powershell-script/\",\n",
        "\"https://insights.blackcoffer.com/design-and-develop-jenkins-shared-library/\",\n",
        "\"https://insights.blackcoffer.com/design-and-develop-retool-app-for-wholecell-io-and-asana-data-using-their-apis/\",\n",
        "\"https://insights.blackcoffer.com/design-and-develop-a-retool-app-that-will-show-stock-and-crypto-related-information-using-iex-api/\",\n",
        "\"https://insights.blackcoffer.com/crm-monday-com-make-com-to-data-warehouse-to-klipfolio-dashboard/\",\n",
        "\"https://insights.blackcoffer.com/ner-task-using-bert-with-data-in-xml-format/\",\n",
        "\"https://insights.blackcoffer.com/qualtrics-api-integration-using-python/\",\n",
        "\"https://insights.blackcoffer.com/design-and-develop-mlops-framework-for-data-centric-ai/\",\n",
        "\"https://insights.blackcoffer.com/nlp-based-approach-for-data-transformation/\",\n",
        "\"https://insights.blackcoffer.com/an-etl-tool-to-pull-data-from-shiphero-to-google-bigquery-data-warehouse/\",\n",
        "\"https://insights.blackcoffer.com/plaid-financial-analytics-a-data-driven-dashboard-to-generate-insights/\",\n",
        "\"https://insights.blackcoffer.com/recommendation-engine-for-insurance-sector-to-expand-business-in-the-rural-area/\",\n",
        "\"https://insights.blackcoffer.com/data-from-crm-via-zapier-to-google-sheets-dynamic-to-powerbi/\",\n",
        "\"https://insights.blackcoffer.com/data-warehouse-to-google-data-studio-looker-dashboard/\",\n",
        "\"https://insights.blackcoffer.com/crm-monday-com-via-zapier-to-power-bi-dashboard/\",\n",
        "\"https://insights.blackcoffer.com/monday-com-to-kpi-dashboard-to-manage-view-and-generate-insights-from-the-crm-data/\",\n",
        "\"https://insights.blackcoffer.com/data-management-for-a-political-saas-application/\",\n",
        "\"https://insights.blackcoffer.com/google-lsa-ads-google-local-service-ads-etl-tools-and-dashboards/\",\n",
        "\"https://insights.blackcoffer.com/ad-networks-marketing-campaign-data-dashboard-in-looker-google-data-studio/\",\n",
        "\"https://insights.blackcoffer.com/analytical-solution-for-a-tech-firm/\",\n",
        "\"https://insights.blackcoffer.com/ai-solution-for-a-technology-information-and-internet-firm/\",\n",
        "\"https://insights.blackcoffer.com/ai-and-nlp-based-solutions-to-automate-data-discovery-for-venture-capital-and-private-equity-principals/\",\n",
        "\"https://insights.blackcoffer.com/an-etl-solution-for-an-internet-publishing-firm/\",\n",
        "\"https://insights.blackcoffer.com/ai-based-algorithmic-trading-bot-for-forex/\",\n",
        "\"https://insights.blackcoffer.com/equity-waterfalls-model-based-saas-application-for-real-estate-sector/\",\n",
        "\"https://insights.blackcoffer.com/ai-solutions-for-foreign-exchange-an-automated-algo-trading-tool/\",\n",
        "\"https://insights.blackcoffer.com/ai-agent-development-and-deployment-in-jina-ai/\",\n",
        "\"https://insights.blackcoffer.com/golden-record-a-knowledge-graph-database-approach-to-unfold-discovery-using-neo4j/\",\n",
        "\"https://insights.blackcoffer.com/advanced-ai-for-trading-automation/\",\n",
        "\"https://insights.blackcoffer.com/create-a-knowledge-graph-to-provide-real-time-analytics-recommendations-and-a-single-source-of-truth/\",\n",
        "\"https://insights.blackcoffer.com/advanced-ai-for-thermal-person-detection/\",\n",
        "\"https://insights.blackcoffer.com/advanced-ai-for-road-cam-threat-detection/\",\n",
        "\"https://insights.blackcoffer.com/advanced-ai-for-pedestrian-crossing-safety/\",\n",
        "\"https://insights.blackcoffer.com/handgun-detection-using-yolo/\",\n",
        "\"https://insights.blackcoffer.com/using-graph-technology-to-create-single-customer-view/\",\n",
        "\"https://insights.blackcoffer.com/car-detection-in-satellite-images/\",\n",
        "\"https://insights.blackcoffer.com/building-a-physics-informed-neural-network-for-circuit-evaluation/\",\n",
        "\"https://insights.blackcoffer.com/connecting-mongodb-database-to-power-bi-dashboard-dashboard-automation/\",\n",
        "\"https://insights.blackcoffer.com/data-transformation/\",\n",
        "\"https://insights.blackcoffer.com/e-commerce-store-analysis-purchase-behavior-ad-spend-conversion-traffic-etc/\",\n",
        "\"https://insights.blackcoffer.com/kpi-dashboard-for-accountants/\",\n",
        "\"https://insights.blackcoffer.com/return-on-advertising-spend-dashboard-marketing-automation-and-analytics-using-etl-and-dashboard/\",\n",
        "\"https://insights.blackcoffer.com/ranking-customer-behaviours-for-business-strategy/\",\n",
        "\"https://insights.blackcoffer.com/algorithmic-trading-for-multiple-commodities-markets-like-forex-metals-energy-etc/\",\n",
        "\"https://insights.blackcoffer.com/trading-bot-for-forex/\",\n",
        "\"https://insights.blackcoffer.com/python-model-for-the-analysis-of-sector-specific-stock-etfs-for-investment-purposes%ef%bf%bc/\",\n",
        "\"https://insights.blackcoffer.com/medical-classification/\",\n",
        "\"https://insights.blackcoffer.com/design-develop-bert-question-answering-model-explanations-with-visualization/\",\n",
        "\"https://insights.blackcoffer.com/design-and-develop-solution-to-anomaly-detection-classification-problems/\",\n",
        "\"https://insights.blackcoffer.com/an-etl-solution-for-currency-data-to-google-big-query/\",\n",
        "\"https://insights.blackcoffer.com/etl-and-mlops-infrastructure-for-blockchain-analytics/\",\n",
        "\"https://insights.blackcoffer.com/an-agent-based-model-of-a-virtual-power-plant-vpp/\",\n",
        "\"https://insights.blackcoffer.com/transform-api-into-sdk-library-and-widget/\",\n",
        "\"https://insights.blackcoffer.com/integration-of-a-product-to-a-cloud-based-crm-platform/\",\n",
        "\"https://insights.blackcoffer.com/a-web-based-dashboard-for-the-filtered-data-retrieval-of-land-records/\",\n",
        "\"https://insights.blackcoffer.com/integration-of-video-conferencing-data-to-the-existing-web-app/\",\n",
        "\"https://insights.blackcoffer.com/design-develop-an-app-in-retool-which-shows-the-progress-of-the-added-video/\",\n",
        "\"https://insights.blackcoffer.com/auvik-connectwise-integration-in-grafana/\",\n",
        "\"https://insights.blackcoffer.com/data-integration-and-big-data-performance-using-elk-stack/\",\n",
        "\"https://insights.blackcoffer.com/web-data-connector/\",\n",
        "\"https://insights.blackcoffer.com/an-app-for-updating-the-email-id-of-the-user-and-stripe-refund-tool-using-retool/\",\n",
        "\"https://insights.blackcoffer.com/an-ai-ml-based-web-application-that-detects-the-correctness-of-text-in-a-given-video/\",\n",
        "\"https://insights.blackcoffer.com/website-tracking-and-insights-using-google-analytics-google-tag-manager/\",\n",
        "\"https://insights.blackcoffer.com/dashboard-to-track-the-analytics-of-the-website-using-google-analytics-and-google-tag-manager/\",\n",
        "\"https://insights.blackcoffer.com/power-bi-dashboard-on-operations-transactions-and-marketing-embedding-the-dashboard-to-web-app/\",\n",
        "\"https://insights.blackcoffer.com/nft-data-automation-looksrare-and-etl-tool/\",\n",
        "\"https://insights.blackcoffer.com/optimize-the-data-scraper-program-to-easily-accommodate-large-files-and-solve-oom-errors/\",\n",
        "\"https://insights.blackcoffer.com/making-a-robust-way-to-sync-data-from-airtables-to-mongodb-using-python-etl-solution/\",\n",
        "\"https://insights.blackcoffer.com/incident-duration-prediction-infrastructure-and-real-estate/\",\n",
        "\"https://insights.blackcoffer.com/statistical-data-analysis-of-reinforced-concrete/\",\n",
        "\"https://insights.blackcoffer.com/database-normalization-segmentation-with-google-data-studio-dashboard-insights/\",\n",
        "\"https://insights.blackcoffer.com/power-bi-dashboard-to-drive-insights-from-complex-data-to-generate-business-insights/\",\n",
        "\"https://insights.blackcoffer.com/real-time-dashboard-to-monitor-infrastructure-activity-and-machines/\",\n",
        "\"https://insights.blackcoffer.com/electric-vehicles-ev-load-management-system-to-forecast-energy-demand/\",\n",
        "\"https://insights.blackcoffer.com/power-bi-data-driven-map-dashboard/\",\n",
        "\"https://insights.blackcoffer.com/google-local-service-ads-lsa-leads-dashboard/\",\n",
        "\"https://insights.blackcoffer.com/aws-lex-voice-and-chatbot/\",\n",
        "\"https://insights.blackcoffer.com/metabridges-api-decentraland-integration/\",\n",
        "\"https://insights.blackcoffer.com/microsoft-azure-chatbot-with-luis-language-understanding/\",\n",
        "\"https://insights.blackcoffer.com/impact-of-news-media-and-press-on-innovation-startups-and-investments/\",\n",
        "\"https://insights.blackcoffer.com/aws-quicksight-reporting-dashboard/\",\n",
        "\"https://insights.blackcoffer.com/google-data-studio-dashboard-for-marketing-ads-and-traction-data/\",\n",
        "\"https://insights.blackcoffer.com/gangala-in-e-commerce-big-data-etl-elt-solution-and-data-warehouse/\",\n",
        "\"https://insights.blackcoffer.com/big-data-solution-to-an-online-multivendor-marketplace-ecommerce-business/\",\n",
        "\"https://insights.blackcoffer.com/creating-a-custom-report-and-dashboard-using-the-data-got-from-atera-api/\",\n",
        "\"https://insights.blackcoffer.com/azure-data-lake-and-power-bi-dashboard/\",\n",
        "\"https://insights.blackcoffer.com/google-data-studio-pipeline-with-gcp-mysql/\",\n",
        "\"https://insights.blackcoffer.com/quickbooks-dashboard-to-find-patterns-in-finance-sales-and-forecasts/\",\n",
        "\"https://insights.blackcoffer.com/marketing-sales-and-financial-data-business-dashboard-wink-report/\",\n",
        "\"https://insights.blackcoffer.com/react-native-apps-in-the-development-portfolio/\",\n",
        "\"https://insights.blackcoffer.com/a-leading-firm-website-seo-optimization/\",\n",
        "\"https://insights.blackcoffer.com/a-leading-hospitality-firm-in-the-usa-website-seo-optimization/\",\n",
        "\"https://insights.blackcoffer.com/a-leading-firm-in-the-usa-website-seo-optimization/\",\n",
        "\"https://insights.blackcoffer.com/a-leading-musical-instrumental-website-seo-optimization/\",\n",
        "\"https://insights.blackcoffer.com/a-leading-firm-in-the-usa-seo-and-website-optimization/\",\n",
        "\"https://insights.blackcoffer.com/immigration-datawarehouse-ai-based-recommendations/\",\n",
        "\"https://insights.blackcoffer.com/lipsync-automation-for-celebrities-and-influencers/\",\n",
        "\"https://insights.blackcoffer.com/key-audit-matters-predictive-modeling/\",\n",
        "\"https://insights.blackcoffer.com/splitting-of-songs-into-its-vocals-and-instrumental/\",\n",
        "\"https://insights.blackcoffer.com/ai-and-ml-technologies-to-evaluate-learning-assessments/\",\n",
        "\"https://insights.blackcoffer.com/datawarehouse-and-recommendations-engine-for-airbnb/\",\n",
        "\"https://insights.blackcoffer.com/real-estate-data-warehouse/\",\n",
        "\"https://insights.blackcoffer.com/traction-dashboards-of-marketing-campaigns-and-posts/\",\n",
        "\"https://insights.blackcoffer.com/google-local-service-ads-lsa-data-warehouse/\",\n",
        "\"https://insights.blackcoffer.com/google-local-service-ads-missed-calls-and-messages-automation-tool/\",\n",
        "\"https://insights.blackcoffer.com/marketing-ads-leads-call-status-data-tool-to-bigquery/\",\n",
        "\"https://insights.blackcoffer.com/marketing-analytics-to-automate-leads-call-status-and-reporting/\",\n",
        "\"https://insights.blackcoffer.com/callrail-analytics-leads-report-alert/\",\n",
        "\"https://insights.blackcoffer.com/marketing-automation-tool-to-notify-lead-details-to-clients-over-email-and-phone/\",\n",
        "\"https://insights.blackcoffer.com/data-etl-local-service-ads-leads-to-bigquery/\",\n",
        "\"https://insights.blackcoffer.com/marbles-stimulation-using-python/\",\n",
        "\"https://insights.blackcoffer.com/stocktwits-data-structurization/\",\n",
        "\"https://insights.blackcoffer.com/sentimental-analysis-on-shareholder-letter-of-companies/\",\n",
        "\"https://insights.blackcoffer.com/population-and-community-survey-of-america/\",\n",
        "\"https://insights.blackcoffer.com/google-lsa-api-data-automation-and-dashboarding/\",\n",
        "\"https://insights.blackcoffer.com/healthcare-data-analysis/\",\n",
        "\"https://insights.blackcoffer.com/budget-sales-kpi-dashboard-using-power-bi/\",\n",
        "\"https://insights.blackcoffer.com/amazon-buy-bot-an-automation-ai-tool-to-auto-checkouts/\"\n",
        "]\n",
        "\n",
        "\n",
        "# Loop through the URLs and download content\n",
        "for i,url in enumerate(urls, start=2011):\n",
        "    res=requests.get(url)\n",
        "    soup=BeautifulSoup(res.content, 'html.parser')\n",
        "\n",
        "    #Extract the title and main content\n",
        "    title=soup.find('title').get_text()\n",
        "    con=soup.find('div',class_='td-post-content').get_text()\n",
        "\n",
        "    #print(main_content)\n",
        "\n",
        "    #Save the content into a text file\n",
        "    with open(f'bctech{i}.txt', 'w', encoding='utf-8') as file:\n",
        "        file.write(f'Title: {title}\\n\\n')\n",
        "        file.write(con)\n",
        "\n",
        "print(\"Content added to text files\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6uSl-4Y-D_Qd",
        "outputId": "aa40e3f2-5734-42a3-f1aa-deeb798be673"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content added to text files\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECKING NLTK PACKAGES AND TOKENIZATION FOR PREPROCESSING\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import re\n",
        "\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5LuRjkmybIR",
        "outputId": "197e0db8-37a4-48d1-d567-057b2f3558b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#CHECK FOR 1 FILE\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize,sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import re\n",
        "\n",
        "def read_stopwords(paths):\n",
        "  stop=set()\n",
        "  for p in paths:\n",
        "    with open(p,'r',encoding='utf-8',errors='ignore') as file:\n",
        "      words=file.read().splitlines()\n",
        "      stop.update(words)\n",
        "  return stop\n",
        "\n",
        "\n",
        "def preprocess(text,stop):\n",
        "  sentences=sent_tokenize(text)\n",
        "  words=[]\n",
        "  for sent in sentences:\n",
        "        sent=re.sub(r'[^\\w\\s]', '', sent)\n",
        "        tokens = word_tokenize(sent.lower())\n",
        "        filtered_tk = [word for word in tokens if word not in stop]\n",
        "        words.extend(filtered_tk)\n",
        "  return words\n",
        "\n",
        "def sentiment_cal(words,pos,neg):\n",
        "  pos_score=sum(1 for word in words if word in pos)\n",
        "  neg_score=sum(1 for word in words if word in neg)\n",
        "  return  pos_score,neg_score\n",
        "\n",
        "\n",
        "def syllable_cnt(word):\n",
        "    vowels='aeiou'\n",
        "    count=0\n",
        "    prev_vowel=False\n",
        "    for char in word.lower():\n",
        "        if char in vowels and not prev_vowel:\n",
        "            count += 1\n",
        "            prev_vowel = True\n",
        "        elif char not in vowels:\n",
        "            prev_vowel = False\n",
        "    if word.endswith('es') or word.endswith('ed'):\n",
        "        count -= 1\n",
        "    if word.endswith('le'):\n",
        "        count += 1\n",
        "    if count == 0:\n",
        "        count = 1\n",
        "    return count\n",
        "\n",
        "\n",
        "def text_analysis(text):\n",
        "    sentences=sent_tokenize(text)\n",
        "    total_words=len(word_tokenize(text))\n",
        "    total_sent=len(sentences)\n",
        "    if total_sent > 0:\n",
        "        avg_s_length=total_words/total_sent\n",
        "    else:\n",
        "        avg_s_length=0\n",
        "\n",
        "    complexw=[word for word in text.split() if syllable_cnt(word)>2]\n",
        "    if total_words>0:\n",
        "        complexw_per=(len(complexw)/total_words)*100\n",
        "    else:\n",
        "         complexw_per=0\n",
        "    fog_index=0.4*(avg_s_length+complexw_per)\n",
        "    return avg_s_length,complexw_per,fog_index\n",
        "\n",
        "\n",
        "stopword_files=['StopWords_Auditor.txt','StopWords_Currencies.txt','StopWords_DatesandNumbers.txt','StopWords_Generic.txt','StopWords_Geographic.txt','StopWords_Names.txt']\n",
        "pos_words_file='positive-words.txt'\n",
        "neg_words_file='negative-words.txt'\n",
        "\n",
        "\n",
        "stopwords=read_stopwords(stopword_files)\n",
        "\n",
        "with open(pos_words_file, 'r',encoding='ISO-8859-1') as file:\n",
        "    pos_words=file.read().splitlines()\n",
        "with open(neg_words_file, 'r',encoding='ISO-8859-1') as file:\n",
        "    neg_words=file.read().splitlines()\n",
        "\n",
        "# Read and process the text from bctech.txt\n",
        "with open('bctech2011.txt', 'r') as file:\n",
        "    text=file.read()\n",
        "\n",
        "clean=preprocess(text, stopwords)\n",
        "\n",
        "pos_score,neg_score=sentiment_cal(clean,pos_words,neg_words)\n",
        "\n",
        "blob=TextBlob(text)\n",
        "polarity_score=blob.sentiment.polarity\n",
        "subjectivity_score=blob.sentiment.subjectivity\n",
        "\n",
        "avg_s_length,complexw_per,fog_index=text_analysis(text)\n",
        "\n",
        "# Output results\n",
        "print(f\"Positive Score: {pos_score}\")\n",
        "print(f\"Negative Score: {neg_score}\")\n",
        "print(f\"Polarity Score: {polarity_score}\")\n",
        "print(f\"Subjectivity Score: {subjectivity_score}\")\n",
        "print(f\"Average Sentence Length: {avg_s_length}\")\n",
        "print(f\"Percentage of Complex Words: {complexw_per}\")\n",
        "print(f\"Fog Index: {fog_index}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TQY2-pROQ36D",
        "outputId": "345d5c37-de11-4376-c2a5-9ce4079e819d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Positive Score: 138\n",
            "Negative Score: 45\n",
            "Polarity Score: 0.08046941376380631\n",
            "Subjectivity Score: 0.39465122105309053\n",
            "Average Sentence Length: 17.327683615819208\n",
            "Percentage of Complex Words: 30.714052820345618\n",
            "Fog Index: 19.216694574465933\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#OFFICIAL VERSION\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from textblob import TextBlob\n",
        "import string\n",
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "def read_stopwords(paths):\n",
        "    stop = set()\n",
        "    for p in paths:\n",
        "        with open(p, 'r', encoding='utf-8', errors='ignore') as file:\n",
        "            words = file.read().splitlines()\n",
        "            stop.update(words)\n",
        "    return stop\n",
        "\n",
        "def preprocess(text, stop):\n",
        "    sentences = sent_tokenize(text)\n",
        "    words = []\n",
        "    for sent in sentences:\n",
        "        sent=re.sub(r'[^\\w\\s]', '', sent)  # Fixed variable name from `sentence` to `sent`\n",
        "        tokens = word_tokenize(sent.lower())\n",
        "        filtered_tokens = [word for word in tokens if word not in stop]\n",
        "        words.extend(filtered_tokens)\n",
        "    return words\n",
        "\n",
        "def sentiment_cal(words, pos, neg):\n",
        "    pos_score = sum(1 for word in words if word in pos)\n",
        "    neg_score = sum(1 for word in words if word in neg)\n",
        "    return pos_score, neg_score\n",
        "\n",
        "def syllable_cnt(word):\n",
        "    vowels = 'aeiou'\n",
        "    count = 0\n",
        "    prev_vowel = False\n",
        "    for char in word.lower():\n",
        "        if char in vowels and not prev_vowel:\n",
        "            count += 1\n",
        "            prev_vowel = True\n",
        "        elif char not in vowels:\n",
        "            prev_vowel = False\n",
        "    if word.endswith('es') or word.endswith('ed'):\n",
        "        count-=1\n",
        "    if word.endswith('le'):\n",
        "        count+=1\n",
        "    if count==0:\n",
        "        count=1\n",
        "    return count\n",
        "\n",
        "def personal_cnt(text):\n",
        "    personalpro = ['i', 'we', 'my', 'ours', 'us']\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    count = sum(1 for word in tokens if word in personalpro)\n",
        "    return count\n",
        "\n",
        "def text_analysis(text):\n",
        "    sentences=sent_tokenize(text)\n",
        "    total_words=len(word_tokenize(text))\n",
        "    words=word_tokenize(text)\n",
        "    total_sent=len(sentences)\n",
        "    if total_sent>0:\n",
        "        avg_s_length=total_words/total_sent\n",
        "        avg_word_length = sum(len(word) for word in words) / total_words\n",
        "        syll_word = sum(syllable_cnt(word) for word in words) / total_words\n",
        "    else:\n",
        "        avg_s_length=0\n",
        "        avg_word_length = 0\n",
        "        syll_word= 0\n",
        "\n",
        "    complexw=[word for word in text.split() if syllable_cnt(word)>2]\n",
        "    if total_words>0:\n",
        "        complexw_per=(len(complexw) / total_words)*100\n",
        "    else:\n",
        "        complexw_per=0\n",
        "    fog_index=0.4 *(avg_s_length + complexw_per)\n",
        "    personal_count=personal_cnt(text)\n",
        "    avg_w_sent=avg_s_length\n",
        "\n",
        "    return (avg_s_length, complexw_per, fog_index, complex_word_count,\n",
        "            total_words, syllable_per_word, personal_count,\n",
        "            avg_word_length,avg_w_sent)\n",
        "\n",
        "stopword_files = [\n",
        "    'StopWords_Auditor.txt', 'StopWords_Currencies.txt',\n",
        "    'StopWords_DatesandNumbers.txt', 'StopWords_Generic.txt',\n",
        "    'StopWords_Geographic.txt', 'StopWords_Names.txt'\n",
        "]\n",
        "pos_words_file = 'positive-words.txt'\n",
        "neg_words_file = 'negative-words.txt'\n",
        "\n",
        "stopwords = read_stopwords(stopword_files)\n",
        "\n",
        "with open(pos_words_file, 'r',encoding='ISO-8859-1') as file:\n",
        "    pos_words = file.read().splitlines()\n",
        "with open(neg_words_file, 'r',encoding='ISO-8859-1') as file:\n",
        "    neg_words = file.read().splitlines()\n",
        "\n",
        "# Initialize a list to store results\n",
        "results = []\n",
        "\n",
        "# Process files from bctech2011.txt to bctech2157.txt\n",
        "for i in range(2011, 2158):\n",
        "    filename = f'bctech{i}.txt'\n",
        "    try:\n",
        "        with open(filename, 'r') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Preprocess the text\n",
        "        clean = preprocess(text, stopwords)\n",
        "\n",
        "        # Perform sentiment analysis\n",
        "        pos_score, neg_score = sentiment_cal(clean, pos_words, neg_words)\n",
        "\n",
        "        # Perform text analysis\n",
        "        blob = TextBlob(text)\n",
        "        polarity_score = blob.sentiment.polarity\n",
        "        subjectivity_score = blob.sentiment.subjectivity\n",
        "\n",
        "\n",
        "        avg_s_length, complexw_per, fog_index, complex_word_count, word_count, syllable_per_word, per_pronoun_count, avg_word_length, avg_w_sent = text_analysis(text)\n",
        "\n",
        "\n",
        "        # Store results for the current file\n",
        "        results.append({\n",
        "            \"URL_ID\": filename,\n",
        "             \"URL\"  : urls[i - 2011],\n",
        "            \"POSITIVE SCORE\":pos_score,\n",
        "            \"NEGATIVE SCORE\":neg_score,\n",
        "            \"POLARITY SCORE\":polarity_score,\n",
        "            \"SUBJECTIVITY SCORE\":subjectivity_score,\n",
        "            \"AVERAGE SENTENCE LENGTH\":avg_s_length,\n",
        "            \"PERCENTAGE OF COMPLEX WORDS\":complexw_per,\n",
        "            \"FOG INDEX\":fog_index,\n",
        "            \"AVG NUMBER OF WORDS PER SENTENCE\":avg_w_sent,\n",
        "            \"COMPLEX WORD COUNT\":complex_word_count,\n",
        "            \"WORD COUNT\":word_count,\n",
        "            \"SYLLABLE PER WORD\":syllable_per_word,\n",
        "            \"PERSONAL PRONOUNS\":per_pronoun_count,\n",
        "            \"AVG WORD LENGTH\":avg_word_length\n",
        "        })\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"{filename} not found. Skipping.\")\n",
        "\n",
        "# Convert results to DataFrame\n",
        "df1=pd.DataFrame(results)\n",
        "\n",
        "# Load the existing Excel file\n",
        "path='Output DS final.xlsx'\n",
        "try:\n",
        "    df2=pd.read_excel(path,sheet_name='Sheet1')\n",
        "    df3=pd.concat([df2,df1], ignore_index=True)\n",
        "except FileNotFoundError:\n",
        "    df3=df1\n",
        "\n",
        "# Write results to Excel file\n",
        "df3.to_excel(path, index=False, sheet_name='Sheet1')\n",
        "\n",
        "print(\"Written in excel\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-Fq-60TD3Xk",
        "outputId": "8e2e71e8-dc3b-4873-bc02-e0bd75f5db14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written in excel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "for i in range(2154,2158):\n",
        " files.download(f'bctech{i}.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "JcHs9g_kQdvV",
        "outputId": "bc6e6beb-ee0b-4999-e8b0-98afbdfc04c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4ec61236-458f-4726-8cbf-81b3a7ce7665\", \"bctech2154.txt\", 9124)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_76411fd8-8379-46e3-932c-32d701152245\", \"bctech2155.txt\", 2571)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_3a0dd96a-d193-40fa-9093-d6f22eb06f20\", \"bctech2156.txt\", 952)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_efd5d441-516e-4f40-a3b1-d955ca5d829d\", \"bctech2157.txt\", 884)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}